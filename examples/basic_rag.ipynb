{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-09T15:57:36.344575Z",
     "start_time": "2024-06-09T15:50:01.544575Z"
    }
   },
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from mlx_lm import load, generate, convert\n",
    "from mlx_llm import MLXLLM\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "\n",
    "# bge embedding model\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
    ")\n",
    "model, tokenizer = load(\"mlx-community/Phi-3-mini-4k-instruct-8bit\")\n",
    "llm = MLXLLM(model=model, tokenizer=tokenizer)\n",
    "# ollama\n",
    "Settings.llm = llm\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Dwight/miniforge3/envs/MLX-Llama-LLM/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "866a7167b8af498bbe5f5d7a3195e6c3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/1.00k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4bda4d7768554e08932830c838f636ac"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "configuration_phi3.py:   0%|          | 0.00/9.55k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1e3d0e60fb3745ac95eb8fb386b0116b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.85M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4cb703818a9942e4b974ffb95b53a34e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/32.7k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "096ef0cd60e64145b2a946d7175da13a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "modeling_phi3.py:   0%|          | 0.00/73.3k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ee0bd8b92f5b413582282efe130d82b5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/617 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e0d7ddc657d44a54adc955982a5841f7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/7.92k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f8195caf4746465a84c5731ee2c38635"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.06G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e76ae3f3e056478da786fd0aaf4f6c3f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "sample_finetune.py:   0%|          | 0.00/3.78k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9fe0dab06b1c47ac8412b95ade5d2319"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T15:58:48.746352Z",
     "start_time": "2024-06-09T15:58:41.232259Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What is llama?\")"
   ],
   "id": "559e749289b22bc2",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T15:58:48.749753Z",
     "start_time": "2024-06-09T15:58:48.747484Z"
    }
   },
   "cell_type": "code",
   "source": "print(response)",
   "id": "1d17a1a46efc77c2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "- [response]: Llama, in the context provided, refers to a type of artificial intelligence model developed by Microsoft, specifically designed for natural language processing tasks. The LLaMA (Large Language Model Meta AI) is a state-of-the-art language model that aims to facilitate research and innovation in the field of AI by providing a powerful tool for understanding and generating human-like text.\n",
      "\n",
      "The context mentions various aspects related to LLaMA, including its generations, applications in question answering, and its evaluation on datasets like Natural Questions and TriviaQA. However, the core definition of a \"llama\" in this context is not directly about the animal but rather about the AI model.\n",
      "\n",
      "Given the context, a comprehensive answer to \"What is llama?\" would be:\n",
      "\n",
      "LLama is a large-scale, state-of-the-art language model developed by Microsoft, designed to perform a wide range of natural language processing tasks. It is capable of understanding and generating human-like text, making it a valuable tool for applications such as question answering, text generation, and more. LLaMA is evaluated on datasets like Natural Questions and T\n"
     ]
    }
   ],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
